
### **Testing Strategy**

- **Purpose:** The purpose of writing automated test scripts is to verify the functional and non-functional requirements efficiently. This improves verification accuracy by reducing human error for repetative tasks and will allow for an increased feature deployment rate. For us, untested code is “dead code”, and we must by default assume that that portion of the system is unverified. Our team must adopt a hierarchical approach to verification through testing, in which the lowest components of the hierarchy are unit tests, followed by integration tests, followed by system tests. The objective will be to make the majority of the tests automated, so that the majority of the verification can take place with a single terminal command. Ideally, the unit tests of a given category will be run before the integration tests of that category, and the integration tests before the system tests. Top level integration tests and system tests likely will have manual components to them, since the UI will become a primary interface. Overall, the test suite is designed to provide a quick, repeatable method to verify the behavior of components in a systematic way.
- **Execution Order:** All tests shall be run in an upward fashion, where unit tests are executed prior to small integration tests, and small integration tests prior to large integration tests.
- **Functional Testing:** Most tests will be written to ensure that the correct outputs are produced.
- **Non-Functional Testing:** Some tests will be written to evaluate characteristics of our system that go beyond I/O.
    1. *Security*: Much of our security testing will be through our unit and integration testing of our authentication and encryption protocols. A system test can exist where the tester mocks a malicious user attempting to do harm to the system.
    2. *Scalability*: We envision two main scalability tests. One of these tests will be a database subsystem integration test, in which we introduce large amounts of synthetic data to verify storage limits and performance speed. The second test will focus on user traffic, and be designed to verify that the host platform will be able to serve an adequate number of users at one time.
    3. *Performance*: This metric will largely apply to the brain games themselves. We will have system tests in which the user manually plays the game and reports on lag and delays.
    4. *Observability*: Our system will output logs and record user data in a streamlined way that we can observe from the administrative end. These records of data can be scanned to ensure they store the correct types of data in an integration test that occurs directly after a system transaction.
    5. *Privacy*: This can be verified as a manual test in which a client user must find no way to observe the data of another client user.
    6. *Regulatory*: Regulatory tests will be specifically tailored to organizations such as HIPPA. An example that applies to our project concerns logging database transactions with personal data. Our system will produce the required logs, and our test will verify that the logs contain the correct information.
- **Documentation:** All official verification attempts must be documented with indication of success and failure for individual tests.